{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609316b",
   "metadata": {},
   "source": [
    "# Tensorflow with Keras MNIST using Optimized Densely Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f14b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea360b1d",
   "metadata": {},
   "source": [
    "# Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089d77c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4c94d",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26453550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading entire mnist dataset\n",
    "# 16.7% of the entire dataset is test data\n",
    "# training data: 60,000 images\n",
    "# testing data: 10,000 images\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# Splitting the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15)\n",
    "\n",
    "# Old code:\n",
    "# The lines where the .reshape() method is used reshapes the data to fit the model.\n",
    "# These lines are not neccessary because \"tf.keras.layers.Flatten(input_shape=(28, 28, 1)\"...\n",
    "# reshapes the data in the \"TF_KERAS_SEQUENTIAL_MODEL_WRAP function...\n",
    "# further downstream to this code.\n",
    "# These lines of code are included here as a reference for an alternative to reshaping data...\n",
    "# for the model that will be used. \n",
    "\n",
    "#X_train = X_train.reshape((X_train.shape[0], 28 * 28))\n",
    "#X_val = X_val.reshape((X_val.shape[0], 28 * 28))\n",
    "#X_test = X_test.reshape((X_test.shape[0], 28 * 28))\n",
    "\n",
    "\n",
    "\n",
    "# Scale the pixel values to be between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_val = X_val.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521895d",
   "metadata": {},
   "source": [
    "# Model created (using helper function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c48f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes regarding \"TF_KERAS_SEQUENTIAL_MODEL_WRAP\" function:\n",
    "\n",
    "# 784 is the number of input features for the model...\n",
    "# since each image is 28 by 28 pixels which equals 784\n",
    "\n",
    "#the hidden layers can be of different size\n",
    "\n",
    "#Regarding \"Flatten\" layer:\n",
    "# each sample is 28x28x1 pixels making them each a rank no.3 tensor\n",
    "# flattening tensor inputs into vectors to feed neural network\n",
    "# this flattening is not needed when working with CNNs.\n",
    "\n",
    "# tf.keras.layers.Dense: output = activation(dot(input, weight) + bias)\n",
    "\n",
    "# softmax activation is used for final \"Dense\" layer because for classification the activation function must transform the values propagated to this layer into probabilities\n",
    "\n",
    "# Regarding specification of optimizer and cost function:\n",
    "# using a loss specifically used for classifiers is best practice\n",
    "# the loss/cost function sparse_categorical_crossentropy appies one-hot encoding to the data\n",
    "# the model and optimizer expect the output shape to match the target shape in a one-hot encoded format\n",
    "\n",
    "\n",
    "\n",
    "def TF_KERAS_SEQUENTIAL_MODEL_WRAP(input_size=784, \n",
    "                                   output_size=10, \n",
    "                                   hidden_layer_size=275,\n",
    "                                   dense_layer_1_activation='relu',\n",
    "                                   dense_layer_2_activation='relu',\n",
    "                                   dense_layer_3_activation='softmax',\n",
    "                                   optimizer='ADAM',\n",
    "                                   loss='categorical_crossentropy'\n",
    "                                   ):\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28, 1)), \n",
    "            tf.keras.layers.Dense(hidden_layer_size, activation=dense_layer_1_activation), \n",
    "            tf.keras.layers.Dense(hidden_layer_size, activation=dense_layer_2_activation), \n",
    "            tf.keras.layers.Dense(output_size, activation=dense_layer_3_activation) \n",
    "            ])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#Important note:\n",
    "#The hyperparameters specified in the creation of this helper function are\n",
    "#here for function testing. This hardcoding will change for the allowance of the use of GridSearchCV later on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671be55",
   "metadata": {},
   "source": [
    "# Defining grid search parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c336b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing GridSearchCV() fucntionality with 'optimizer' parameter options.\n",
    "param_grid = {'optimizer': ['RMSprop', 'ADAM', 'Nadam']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca520dcf",
   "metadata": {},
   "source": [
    "# Setting data to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3db1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data \"to_categorical\" \n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80f396",
   "metadata": {},
   "source": [
    "# Wrapping model with KerasClassifier from SciKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c89fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes regarding \"KerasClassifier\" hyperparameters:\n",
    "# Number of epochs set is arbitrary\n",
    "# batch_size is set for mini-batch gradient (which will be used to train the model)\n",
    "# these hyperparameter can be fined-tuned to attempt to improve the model\n",
    "\n",
    "\n",
    "# Set early stopping: monitors loss and stop training proccess the first time loss starts to increase\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "# wrapping model with KerasClassifer using helper function: \"TF_KERAS_SEQUENTIAL_MODEL_WRAP\"\n",
    "kc = KerasClassifier(build_fn=TF_KERAS_SEQUENTIAL_MODEL_WRAP, epochs=6, batch_size=100, verbose=2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d399acd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a37c4bc",
   "metadata": {},
   "source": [
    "## Running Gridsearch and Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad1f036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "408/408 - 2s - loss: 0.2872 - accuracy: 0.9163 - 2s/epoch - 6ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1088 - accuracy: 0.9670 - 2s/epoch - 4ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0687 - accuracy: 0.9787 - 2s/epoch - 4ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0477 - accuracy: 0.9853 - 2s/epoch - 4ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0333 - accuracy: 0.9898 - 2s/epoch - 4ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0270 - accuracy: 0.9909 - 2s/epoch - 4ms/step\n",
      "102/102 - 0s - 290ms/epoch - 3ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 2s - loss: 0.2814 - accuracy: 0.9183 - 2s/epoch - 6ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1080 - accuracy: 0.9673 - 2s/epoch - 4ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0686 - accuracy: 0.9794 - 2s/epoch - 4ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0473 - accuracy: 0.9852 - 2s/epoch - 4ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0343 - accuracy: 0.9892 - 2s/epoch - 4ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0260 - accuracy: 0.9921 - 2s/epoch - 4ms/step\n",
      "102/102 - 0s - 263ms/epoch - 3ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 2s - loss: 0.2907 - accuracy: 0.9161 - 2s/epoch - 5ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1116 - accuracy: 0.9661 - 2s/epoch - 4ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0707 - accuracy: 0.9786 - 2s/epoch - 4ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0496 - accuracy: 0.9844 - 2s/epoch - 4ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0332 - accuracy: 0.9898 - 2s/epoch - 4ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0260 - accuracy: 0.9916 - 2s/epoch - 4ms/step\n",
      "102/102 - 0s - 261ms/epoch - 3ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 2s - loss: 0.2935 - accuracy: 0.9135 - 2s/epoch - 5ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1088 - accuracy: 0.9674 - 2s/epoch - 4ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0718 - accuracy: 0.9777 - 2s/epoch - 4ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0483 - accuracy: 0.9852 - 2s/epoch - 4ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0358 - accuracy: 0.9888 - 2s/epoch - 4ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0272 - accuracy: 0.9911 - 2s/epoch - 4ms/step\n",
      "102/102 - 0s - 260ms/epoch - 3ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 2s - loss: 0.2879 - accuracy: 0.9169 - 2s/epoch - 5ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1092 - accuracy: 0.9669 - 2s/epoch - 4ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0686 - accuracy: 0.9793 - 2s/epoch - 4ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0486 - accuracy: 0.9850 - 2s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0367 - accuracy: 0.9889 - 2s/epoch - 5ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0266 - accuracy: 0.9917 - 2s/epoch - 5ms/step\n",
      "102/102 - 0s - 490ms/epoch - 5ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 3s - loss: 0.2866 - accuracy: 0.9157 - 3s/epoch - 9ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1088 - accuracy: 0.9676 - 2s/epoch - 5ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0704 - accuracy: 0.9790 - 2s/epoch - 5ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0477 - accuracy: 0.9853 - 2s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0348 - accuracy: 0.9894 - 2s/epoch - 5ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0274 - accuracy: 0.9913 - 2s/epoch - 5ms/step\n",
      "102/102 - 0s - 443ms/epoch - 4ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 3s - loss: 0.2825 - accuracy: 0.9176 - 3s/epoch - 8ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 3s - loss: 0.1079 - accuracy: 0.9669 - 3s/epoch - 6ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0673 - accuracy: 0.9793 - 2s/epoch - 5ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0483 - accuracy: 0.9850 - 2s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0357 - accuracy: 0.9889 - 2s/epoch - 5ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0266 - accuracy: 0.9912 - 2s/epoch - 5ms/step\n",
      "102/102 - 0s - 441ms/epoch - 4ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 5s - loss: 0.2915 - accuracy: 0.9158 - 5s/epoch - 13ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1108 - accuracy: 0.9667 - 2s/epoch - 5ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0713 - accuracy: 0.9779 - 2s/epoch - 5ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0507 - accuracy: 0.9847 - 2s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0342 - accuracy: 0.9894 - 2s/epoch - 5ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0288 - accuracy: 0.9909 - 2s/epoch - 5ms/step\n",
      "102/102 - 1s - 1s/epoch - 13ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 3s - loss: 0.2862 - accuracy: 0.9158 - 3s/epoch - 8ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1077 - accuracy: 0.9670 - 2s/epoch - 5ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0659 - accuracy: 0.9803 - 2s/epoch - 5ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0472 - accuracy: 0.9854 - 2s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0349 - accuracy: 0.9887 - 2s/epoch - 5ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0263 - accuracy: 0.9916 - 2s/epoch - 5ms/step\n",
      "102/102 - 0s - 367ms/epoch - 4ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 3s - loss: 0.2856 - accuracy: 0.9175 - 3s/epoch - 8ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1092 - accuracy: 0.9682 - 2s/epoch - 5ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0686 - accuracy: 0.9789 - 2s/epoch - 6ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0480 - accuracy: 0.9846 - 2s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0341 - accuracy: 0.9897 - 2s/epoch - 5ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0257 - accuracy: 0.9920 - 2s/epoch - 4ms/step\n",
      "102/102 - 0s - 333ms/epoch - 3ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 3s - loss: 0.2874 - accuracy: 0.9159 - 3s/epoch - 7ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1106 - accuracy: 0.9664 - 2s/epoch - 5ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0681 - accuracy: 0.9785 - 2s/epoch - 5ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0475 - accuracy: 0.9859 - 2s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0341 - accuracy: 0.9895 - 2s/epoch - 5ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0276 - accuracy: 0.9911 - 2s/epoch - 5ms/step\n",
      "102/102 - 0s - 376ms/epoch - 4ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 3s - loss: 0.2799 - accuracy: 0.9191 - 3s/epoch - 8ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1091 - accuracy: 0.9670 - 2s/epoch - 6ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0669 - accuracy: 0.9792 - 2s/epoch - 5ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0475 - accuracy: 0.9855 - 2s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0361 - accuracy: 0.9888 - 2s/epoch - 5ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0257 - accuracy: 0.9919 - 2s/epoch - 5ms/step\n",
      "102/102 - 0s - 409ms/epoch - 4ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 8s - loss: 0.2919 - accuracy: 0.9165 - 8s/epoch - 20ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1061 - accuracy: 0.9687 - 2s/epoch - 6ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 3s - loss: 0.0701 - accuracy: 0.9787 - 3s/epoch - 6ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0485 - accuracy: 0.9853 - 2s/epoch - 6ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0343 - accuracy: 0.9893 - 2s/epoch - 6ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0269 - accuracy: 0.9914 - 2s/epoch - 5ms/step\n",
      "102/102 - 0s - 445ms/epoch - 4ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 4s - loss: 0.2846 - accuracy: 0.9185 - 4s/epoch - 10ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1052 - accuracy: 0.9678 - 2s/epoch - 6ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0676 - accuracy: 0.9793 - 2s/epoch - 6ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 2s - loss: 0.0469 - accuracy: 0.9854 - 2s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 2s - loss: 0.0361 - accuracy: 0.9880 - 2s/epoch - 6ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0226 - accuracy: 0.9931 - 2s/epoch - 6ms/step\n",
      "102/102 - 0s - 301ms/epoch - 3ms/step\n",
      "Epoch 1/6\n",
      "408/408 - 4s - loss: 0.2852 - accuracy: 0.9195 - 4s/epoch - 9ms/step\n",
      "Epoch 2/6\n",
      "408/408 - 2s - loss: 0.1119 - accuracy: 0.9667 - 2s/epoch - 6ms/step\n",
      "Epoch 3/6\n",
      "408/408 - 2s - loss: 0.0697 - accuracy: 0.9787 - 2s/epoch - 5ms/step\n",
      "Epoch 4/6\n",
      "408/408 - 3s - loss: 0.0468 - accuracy: 0.9850 - 3s/epoch - 6ms/step\n",
      "Epoch 5/6\n",
      "408/408 - 3s - loss: 0.0385 - accuracy: 0.9879 - 3s/epoch - 6ms/step\n",
      "Epoch 6/6\n",
      "408/408 - 2s - loss: 0.0263 - accuracy: 0.9917 - 2s/epoch - 6ms/step\n",
      "102/102 - 0s - 484ms/epoch - 5ms/step\n",
      "Epoch 1/6\n",
      "510/510 - 4s - loss: 0.2635 - accuracy: 0.9229 - 4s/epoch - 8ms/step\n",
      "Epoch 2/6\n",
      "510/510 - 3s - loss: 0.0969 - accuracy: 0.9704 - 3s/epoch - 6ms/step\n",
      "Epoch 3/6\n",
      "510/510 - 3s - loss: 0.0608 - accuracy: 0.9813 - 3s/epoch - 6ms/step\n",
      "Epoch 4/6\n",
      "510/510 - 3s - loss: 0.0425 - accuracy: 0.9868 - 3s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "510/510 - 3s - loss: 0.0320 - accuracy: 0.9902 - 3s/epoch - 5ms/step\n",
      "Epoch 6/6\n",
      "510/510 - 3s - loss: 0.0258 - accuracy: 0.9917 - 3s/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=kc, param_grid=param_grid, cv=5)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# The inside info on each epoch:\n",
    "# 1. at the start of each epoch, training loss set to 0\n",
    "# 2. algo iterates over the preset num of batches, from train_data\n",
    "# 3. weights and bias update as many times as there are batches\n",
    "# 4. user recieves value for loss function, which indicates how the training is going\n",
    "# 5. user additionally recieves training accuracy \n",
    "# 6. at the end of the epoch, algo will forward propagate entire validation dataset\n",
    "# fin. training ends when max number of epochs reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e1f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inside info on each epoch:\n",
    "# 1. at the start of each epoch, training loss set to 0\n",
    "# 2. algo iterates over the preset num of batches, from train_data\n",
    "# 3. weights and bias update as many times as there are batches\n",
    "# 4. user recieves value for loss function, which indicates how the training is going\n",
    "# 5. user additionally recieves training accuracy \n",
    "# 6. at the end of the epoch, algo will forward propagate entire validation dataset\n",
    "# fin. training ends when max number of epochs reached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d47250",
   "metadata": {},
   "source": [
    "# Note regarding model assessment from training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf0c26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing our model:\n",
    "# we look at the validation accuracy to see if model is overfitting\n",
    "# validation accuracy is the true accuracy of the model\n",
    "# to assess the overall accuracy of the model we lok at the validation accuracy for the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1969e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7fdfe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 1s - 660ms/epoch - 7ms/step\n",
      "1. Test score for best estimator: 0.9784\n",
      "\n",
      "\n",
      "2. Best estimator found:  KerasClassifier(\n",
      "\tmodel=None\n",
      "\tbuild_fn=<function TF_KERAS_SEQUENTIAL_MODEL_WRAP at 0x000002CF2B1028C0>\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=RMSprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=100\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=2\n",
      "\tcallbacks=[<keras.src.callbacks.EarlyStopping object at 0x000002CF2B1083D0>]\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=6\n",
      "\tclass_weight=None\n",
      ")\n",
      "\n",
      "\n",
      "3. Best parameters found:  {'optimizer': 'RMSprop'}\n"
     ]
    }
   ],
   "source": [
    "# by testing the model accuracy on the test data we have a sanity check which tells us if we tuned the hyperparameters to overfit the validation dataset\n",
    "\n",
    "# Note regarding use of the \"score method\":\n",
    "# Since a SciKeras wrapper was used in model training, the \"score\" method is used to evaluate the model...\n",
    "# after passing in test data because SciKeras wrapper objects have no evaluate method.\n",
    "# In the case of using GridSearchCV(), the score fucntion is called on what was found to be the best estimator.\n",
    "\n",
    "# forward propagates test data through the net\n",
    "test_score = grid_result.best_estimator_.score(X_test, y_test)\n",
    "print(\"1. Test score for best estimator:\", test_score)\n",
    "print(\"\\n\")\n",
    "print(\"2. Best estimator found: \", grid_result.best_estimator_)\n",
    "print(\"\\n\")\n",
    "print(\"3. Best parameters found: \", grid_result.best_params_)\n",
    "\n",
    "\n",
    "#old code (without SciKeras wrapper used in this project):\n",
    "#test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8844afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//prints the results with formatting applied, in case user wants to do so//\n",
    "# print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n",
    "\n",
    "#Important Note:\n",
    "# once the model has been tested it is best practice to NOT change it any further than it already is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ae512",
   "metadata": {},
   "source": [
    "# Final Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c237a5",
   "metadata": {},
   "source": [
    "## Building the model plainly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb356674",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # 784 is the number of input features for the model...\n",
    "                 # since each image is 28 by 28 pixels which equals 784\n",
    "    \n",
    "output_size = 10 # there are 10 classes each sample can be classified as \n",
    "\n",
    "#the hidden layers can be of different size\n",
    "hidden_layer_size = 275 # _optimal hyperpara val found_ (through manual search)\n",
    "\n",
    "# Best optimizer found through GridSearchCV():\n",
    "optimizer_best = 'RMSprop'\n",
    "\n",
    "\n",
    "\n",
    "# Building Final Model\n",
    "final_model = tf.keras.Sequential([\n",
    "    # each sample is 28x28x1 pixels making them each a rank no.3 tensor\n",
    "    # flattening tensor inputs into vectors to feed neural network\n",
    "    # this flattening is not needed when working with CNNs.\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense: output = activation(dot(input, weight) + bias)\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # hidden layer no.1\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # hidden layer no.2\n",
    "    \n",
    "    # softmax activation is used here because for classification the activation function must transform the values propagated to this layer into probabilities\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# using a loss specifically used for classifiers is best practice\n",
    "# the loss/cost function sparse_categorical_crossentropy appies one-hot encoding to the data\n",
    "# the model and optimizer expect the output shape to match the target shape in a one-hot encoded format\n",
    "final_model.compile(optimizer=optimizer_best, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb1138",
   "metadata": {},
   "source": [
    "## Final model training using plain model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7283219",
   "metadata": {},
   "source": [
    "### Final data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "833ddf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation of X and Y datasets and shuffling to randomly in replicatable fashion \n",
    "np.random.seed(69)\n",
    "X = np.concatenate((X_train, X_val, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_val, y_test), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Generating a random permutation of indices using the permutation function\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Useing the permutation to index X and y arrays\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# This ensures that both arrays are shuffled in the same order, preserving the correspondence between the features and labels.\n",
    "# After running this code, both the X and y arrays will be correctly shuffled and ready for final model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fdffac",
   "metadata": {},
   "source": [
    "### Setting variable that will be used for the fitting of the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c566e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores the number of epochs set for training\n",
    "# number is arbitrary\n",
    "NUM_EPOCHS = 6 # _optimized hyper para val_\n",
    "\n",
    "\n",
    "\n",
    "#monitors validation loss and stop training proccess the first time the validation loss starts increasing\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae0606",
   "metadata": {},
   "source": [
    "### Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de0246b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1750/1750 - 11s - loss: 0.1971 - accuracy: 0.9401 - val_loss: 0.1052 - val_accuracy: 0.9670 - 11s/epoch - 6ms/step\n",
      "Epoch 2/6\n",
      "1750/1750 - 9s - loss: 0.0899 - accuracy: 0.9738 - val_loss: 0.0994 - val_accuracy: 0.9721 - 9s/epoch - 5ms/step\n",
      "Epoch 3/6\n",
      "1750/1750 - 8s - loss: 0.0629 - accuracy: 0.9817 - val_loss: 0.1258 - val_accuracy: 0.9685 - 8s/epoch - 5ms/step\n",
      "Epoch 4/6\n",
      "1750/1750 - 9s - loss: 0.0498 - accuracy: 0.9864 - val_loss: 0.0944 - val_accuracy: 0.9756 - 9s/epoch - 5ms/step\n",
      "Epoch 5/6\n",
      "1750/1750 - 9s - loss: 0.0389 - accuracy: 0.9888 - val_loss: 0.1080 - val_accuracy: 0.9765 - 9s/epoch - 5ms/step\n",
      "Epoch 6/6\n",
      "1750/1750 - 9s - loss: 0.0313 - accuracy: 0.9909 - val_loss: 0.1027 - val_accuracy: 0.9772 - 9s/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2cf280dcf10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(X, y, epochs=NUM_EPOCHS, callbacks=[early_stopping], validation_split=0.2, verbose =2)\n",
    "\n",
    "\n",
    "# The inside info on each epoch:\n",
    "# 1. at the start of each epoch, training loss set to 0\n",
    "# 2. algo iterates over the preset num of batches, from train_data\n",
    "# 3. weights and bias update as many times as there are batches\n",
    "# 4. user recieves value for loss function, which indicates how the training is going\n",
    "# 5. user additionally recieves training accuracy \n",
    "# 6. at the end of the epoch, algo will forward propagate entire validation dataset\n",
    "# fin. training ends when max number of epochs reached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf2c2a",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3aa93af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save('MNIST_DNN_FINAL.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debb7b9",
   "metadata": {},
   "source": [
    "# Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc47eb",
   "metadata": {},
   "source": [
    "It is best to save a keras-based model as shown. It is not best to use joblib. Joblib should be used to save machine learning models only. Keras models are too complex and have mechanisms in place to handle this complexity which joblib does not have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1126422",
   "metadata": {},
   "source": [
    "Using GridSearchCV() for DNN hyperparameter search and cross-validation is sucessfully demonstrated here.\n",
    "The next step would be to expand the hyperparameters dictonary to search through other hyperparameters.\n",
    "Since this the more hyperparameters are searched for the more computational resources will need to be used, I suggest expanding the use of GridSearchCV() using GPU through Google Colab or Amazon SageMaker Studio Lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
