{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609316b",
   "metadata": {},
   "source": [
    "# Tensorflow with Keras MNIST using Optimized Densely Connected Neural Network and with no GridsearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea360b1d",
   "metadata": {},
   "source": [
    "# Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089d77c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chang.LAPTOP-KLP71L1N\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.12.2 when it was built against 1.12.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4c94d",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26453550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading entire mnist dataset\n",
    "# 16.7% of the entire dataset is test data\n",
    "# training data: 60,000 images\n",
    "# testing data: 10,000 images\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# Splitting the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15)\n",
    "\n",
    "# Old code:\n",
    "# The lines where the .reshape() method is used reshapes the data to fit the model.\n",
    "# These lines are not neccessary because \"tf.keras.layers.Flatten(input_shape=(28, 28, 1)\"...\n",
    "# reshapes the data in the \"TF_KERAS_SEQUENTIAL_MODEL_WRAP function...\n",
    "# further downstream to this code.\n",
    "# These lines of code are included here as a reference for an alternative to reshaping data...\n",
    "# for the model that will be used. \n",
    "\n",
    "#X_train = X_train.reshape((X_train.shape[0], 28 * 28))\n",
    "#X_val = X_val.reshape((X_val.shape[0], 28 * 28))\n",
    "#X_test = X_test.reshape((X_test.shape[0], 28 * 28))\n",
    "\n",
    "\n",
    "\n",
    "# Scale the pixel values to be between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_val = X_val.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521895d",
   "metadata": {},
   "source": [
    "# Model created (using helper function) for use in SKLEARN Pipeline through wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c48f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes regarding \"TF_KERAS_SEQUENTIAL_MODEL_WRAP\" function:\n",
    "\n",
    "# 784 is the number of input features for the model...\n",
    "# since each image is 28 by 28 pixels which equals 784\n",
    "\n",
    "#the hidden layers can be of different size\n",
    "\n",
    "#Regarding \"Flatten\" layer:\n",
    "# each sample is 28x28x1 pixels making them each a rank no.3 tensor\n",
    "# flattening tensor inputs into vectors to feed neural network\n",
    "# this flattening is not needed when working with CNNs.\n",
    "\n",
    "# tf.keras.layers.Dense: output = activation(dot(input, weight) + bias)\n",
    "\n",
    "# softmax activation is used for final \"Dense\" layer because for classification the activation function must transform the values propagated to this layer into probabilities\n",
    "\n",
    "# Regarding specification of optimizer and cost function:\n",
    "# using a loss specifically used for classifiers is best practice\n",
    "# the loss/cost function sparse_categorical_crossentropy appies one-hot encoding to the data\n",
    "# the model and optimizer expect the output shape to match the target shape in a one-hot encoded format\n",
    "\n",
    "\n",
    "\n",
    "def TF_KERAS_SEQUENTIAL_MODEL_WRAP(input_size=784, \n",
    "                                   output_size=10, \n",
    "                                   hidden_layer_size=275,\n",
    "                                   dense_layer_1_activation='relu',\n",
    "                                   dense_layer_2_activation='relu',\n",
    "                                   dense_layer_3_activation='softmax',\n",
    "                                   optimizer='ADAM',\n",
    "                                   loss='categorical_crossentropy'\n",
    "                                   ):\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28, 1)), \n",
    "            tf.keras.layers.Dense(hidden_layer_size, activation=dense_layer_1_activation), \n",
    "            tf.keras.layers.Dense(hidden_layer_size, activation=dense_layer_2_activation), \n",
    "            tf.keras.layers.Dense(output_size, activation=dense_layer_3_activation) \n",
    "            ])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#Important note:\n",
    "#The hyperparameters specified in the creation of this helper function are\n",
    "#here for function testing. This hardcoding will change for the allowance of the use of GridSearchCV later on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80f396",
   "metadata": {},
   "source": [
    "# Model Wrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c89fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes regarding \"KerasClassifier\" hyperparameters:\n",
    "# Number of epochs set is arbitrary\n",
    "# batch_size is set for mini-batch gradient (which will be used to train the model)\n",
    "# these hyperparameter can be fined-tuned to attempt to improve the model\n",
    "\n",
    "\n",
    "# wrapping model using \"TF_KERAS_SEQUENTIAL_MODEL_WRAP\"\n",
    "kc = KerasClassifier(build_fn=TF_KERAS_SEQUENTIAL_MODEL_WRAP, epochs=6, batch_size=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d399acd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e1f632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chang.LAPTOP-KLP71L1N\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "510/510 - 12s - loss: 0.2639 - accuracy: 0.9217 - val_loss: 0.1435 - val_accuracy: 0.9561 - 12s/epoch - 23ms/step\n",
      "Epoch 2/6\n",
      "510/510 - 6s - loss: 0.0969 - accuracy: 0.9704 - val_loss: 0.1145 - val_accuracy: 0.9660 - 6s/epoch - 12ms/step\n",
      "Epoch 3/6\n",
      "510/510 - 6s - loss: 0.0639 - accuracy: 0.9799 - val_loss: 0.0916 - val_accuracy: 0.9726 - 6s/epoch - 12ms/step\n",
      "Epoch 4/6\n",
      "510/510 - 6s - loss: 0.0446 - accuracy: 0.9858 - val_loss: 0.0844 - val_accuracy: 0.9754 - 6s/epoch - 12ms/step\n",
      "Epoch 5/6\n",
      "510/510 - 7s - loss: 0.0327 - accuracy: 0.9894 - val_loss: 0.0925 - val_accuracy: 0.9734 - 7s/epoch - 13ms/step\n",
      "Epoch 6/6\n",
      "510/510 - 6s - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.0993 - val_accuracy: 0.9732 - 6s/epoch - 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function TF_KERAS_SEQUENTIAL_MODEL_WRAP at 0x000001A3382D5CF0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=100\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=2\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=6\n",
       "\tclass_weight=None\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function TF_KERAS_SEQUENTIAL_MODEL_WRAP at 0x000001A3382D5CF0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=100\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=2\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=6\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function TF_KERAS_SEQUENTIAL_MODEL_WRAP at 0x000001A3382D5CF0>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=100\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=2\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=6\n",
       "\tclass_weight=None\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set data \"to_categorical\" \n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "\n",
    "#monitors validation loss and stop training proccess the first time the validation loss starts increasing\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "\n",
    "\n",
    "kc.fit(X_train, y_train, callbacks=[early_stopping], validation_data=(X_val, y_val), verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "# The inside info on each epoch:\n",
    "# 1. at the start of each epoch, training loss set to 0\n",
    "# 2. algo iterates over the preset num of batches, from train_data\n",
    "# 3. weights and bias update as many times as there are batches\n",
    "# 4. user recieves value for loss function, which indicates how the training is going\n",
    "# 5. user additionally recieves training accuracy \n",
    "# 6. at the end of the epoch, algo will forward propagate entire validation dataset\n",
    "# fin. training ends when max number of epochs reached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d47250",
   "metadata": {},
   "source": [
    "# Note regarding model assessment from training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf0c26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing our model:\n",
    "# we look at the validation accuracy to see if model is overfitting\n",
    "# validation accuracy is the true accuracy of the model\n",
    "# to assess the overall accuracy of the model we lok at the validation accuracy for the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1969e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fdfe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 1s - 1s/epoch - 11ms/step\n",
      "0.9759\n"
     ]
    }
   ],
   "source": [
    "# by testing the model accuracy on the test data we have a sanity check which tells us if we tuned the hyperparameters to overfit the validation dataset\n",
    "\n",
    "#Note regarding use of the \"score method\":\n",
    "# Since a scikit-learn pipeline was used in model training, the \"score\" method is used to evaluate the model...\n",
    "# after passing in test data because scikit-learn pipeline objects have no evaluate method.\n",
    "\n",
    "# forward propagates test data through the net\n",
    "test_score = kc.score(X_test, y_test)\n",
    "print(test_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#old code (without scikit-learn pipeline used in this project):\n",
    "#test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8844afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the results with formatting applied in case user wants to do so\n",
    "# print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n",
    "\n",
    "#Important Note:\n",
    "# once the model has been tested it is best practice to NOT change it any further than it already is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ae512",
   "metadata": {},
   "source": [
    "# Final Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c237a5",
   "metadata": {},
   "source": [
    "## Building the model plainly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb356674",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # 784 is the number of input features for the model...\n",
    "                 # since each image is 28 by 28 pixels which equals 784\n",
    "output_size = 10 # there are 10 classes each sample can be classified as \n",
    "#the hidden layers can be of different size\n",
    "hidden_layer_size = 275 # _optimal hyperpara val found_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_model = tf.keras.Sequential([\n",
    "    # each sample is 28x28x1 pixels making them each a rank no.3 tensor\n",
    "    # flattening tensor inputs into vectors to feed neural network\n",
    "    # this flattening is not needed when working with CNNs.\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense: output = activation(dot(input, weight) + bias)\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # hidden layer no.1\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # hidden layer no.2\n",
    "    \n",
    "    # softmax activation is used here because for classification the activation function must transform the values propagated to this layer into probabilities\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# using a loss specifically used for classifiers is best practice\n",
    "# the loss/cost function sparse_categorical_crossentropy appies one-hot encoding to the data\n",
    "# the model and optimizer expect the output shape to match the target shape in a one-hot encoded format\n",
    "final_model.compile(optimizer='ADAM', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb1138",
   "metadata": {},
   "source": [
    "## Final model training using plain model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7283219",
   "metadata": {},
   "source": [
    "### Final data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833ddf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation of X and Y datasets and shuffling to randomly in replicatable fashion \n",
    "np.random.seed(69)\n",
    "X = np.concatenate((X_train, X_val, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_val, y_test), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Generating a random permutation of indices using the permutation function\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Useing the permutation to index X and y arrays\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# This ensures that both arrays are shuffled in the same order, preserving the correspondence between the features and labels.\n",
    "# After running this code, both the X and y arrays will be correctly shuffled and ready for final model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fdffac",
   "metadata": {},
   "source": [
    "### Setting variable that will be used for the fitting of the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c566e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores the number of epochs set for training\n",
    "# number is arbitrary\n",
    "NUM_EPOCHS = 6 # _optimized hyper para val_\n",
    "\n",
    "\n",
    "\n",
    "#monitors validation loss and stop training proccess the first time the validation loss starts increasing\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae0606",
   "metadata": {},
   "source": [
    "### Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de0246b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1750/1750 - 22s - loss: 0.2005 - accuracy: 0.9392 - val_loss: 0.1147 - val_accuracy: 0.9634 - 22s/epoch - 12ms/step\n",
      "Epoch 2/6\n",
      "1750/1750 - 20s - loss: 0.0832 - accuracy: 0.9740 - val_loss: 0.0836 - val_accuracy: 0.9743 - 20s/epoch - 11ms/step\n",
      "Epoch 3/6\n",
      "1750/1750 - 20s - loss: 0.0569 - accuracy: 0.9824 - val_loss: 0.0942 - val_accuracy: 0.9719 - 20s/epoch - 12ms/step\n",
      "Epoch 4/6\n",
      "1750/1750 - 16s - loss: 0.0411 - accuracy: 0.9864 - val_loss: 0.0795 - val_accuracy: 0.9778 - 16s/epoch - 9ms/step\n",
      "Epoch 5/6\n",
      "1750/1750 - 19s - loss: 0.0339 - accuracy: 0.9879 - val_loss: 0.0927 - val_accuracy: 0.9767 - 19s/epoch - 11ms/step\n",
      "Epoch 6/6\n",
      "1750/1750 - 19s - loss: 0.0300 - accuracy: 0.9904 - val_loss: 0.0890 - val_accuracy: 0.9767 - 19s/epoch - 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a33839b610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(X, y, epochs=NUM_EPOCHS, callbacks=[early_stopping], validation_split=0.2, verbose =2)\n",
    "\n",
    "\n",
    "# The inside info on each epoch:\n",
    "# 1. at the start of each epoch, training loss set to 0\n",
    "# 2. algo iterates over the preset num of batches, from train_data\n",
    "# 3. weights and bias update as many times as there are batches\n",
    "# 4. user recieves value for loss function, which indicates how the training is going\n",
    "# 5. user additionally recieves training accuracy \n",
    "# 6. at the end of the epoch, algo will forward propagate entire validation dataset\n",
    "# fin. training ends when max number of epochs reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aa93af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save('MNIST_DNN_FINAL.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0adc1",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc47eb",
   "metadata": {},
   "source": [
    "It is best to save a keras-based model as shown. It is not best to use joblib. Joblib should be used to save machine learning models only. Keras models are too complex and have mechanisms in place to handle this complexity which joblib does not have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352cc38b",
   "metadata": {},
   "source": [
    "However, in our case here, saving the model as a keras model is not working. It may have something to do with the keras wrapper beining used. One potential solution is to do final training on the model with a regularly built DNN. Meaning training the final model on a plain DNN  with the parameters that we are happy with. This should work. The only reason we are using the keras wrapper here is for proof of concept. I just want to know I can set it up so that I can work on a project that requires it. Training a DNN model on MNIST data with grid search and cross-validation. In this project the final model can be built plainly with the best parameters found during gridsearch and cross-validation on the testing model. Same should be done here. For now however, I will unideally save the final model created with joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e06e2a",
   "metadata": {},
   "source": [
    "Update: The final model was build plainly as discussed above. I was able to save it as a keras file no problem. This makes this project a good foundation for testing the use of scikit-learn's GridSearchCV on a keras DNN model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
